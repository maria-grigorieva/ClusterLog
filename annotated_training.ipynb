{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.8.10 64-bit ('conda': virtualenv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Annotating the messages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from torch.cuda import is_available\n",
    "\n",
    "is_available()"
   ]
  },
  {
   "source": [
    "Here we're writing the patterns into a file for further annotation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns = []\n",
    "# for pattern_list in result['patterns']:\n",
    "#     for pattern in pattern_list:\n",
    "#         patterns.append(pattern)\n",
    "\n",
    "# with open(\"../datasets/patterns.txt\", 'w') as pattern_file:\n",
    "#     for pattern in patterns[:20]:\n",
    "#         pattern_file.write(pattern.strip() + '\\n')"
   ]
  },
  {
   "source": [
    "At first we need to open the file with annotations and transform it into an adequate form"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "708"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "from json import loads\n",
    "with open('../datasets/admin.jsonl', 'r') as annotated_file:\n",
    "    data = loads(annotated_file.read())\n",
    "\n",
    "raw_annotations = data['label']\n",
    "annotations = []\n",
    "for annotation in raw_annotations:\n",
    "    start, end, label = annotation\n",
    "    annotations.append({'start': start, 'end': end, 'label': label, 'text': data['data'][start:end]})\n",
    "\n",
    "annotations = sorted(annotations, key=lambda x: x['start'])\n",
    "split_content = data['data'].split('\\n')\n",
    "\n",
    "current_offset = 0\n",
    "next_offset = len(split_content[0]) + 1\n",
    "message_number = 0\n",
    "annotated_messages = [{'text': text, 'annotations': []} for text in split_content]\n",
    "for annotation in annotations:\n",
    "    if annotation['start'] >= next_offset and message_number < len(split_content) - 1:\n",
    "        message_number += 1\n",
    "        current_offset = next_offset\n",
    "        next_offset += len(split_content[message_number]) + 1\n",
    "    offset_annotation = {\n",
    "        'start': annotation['start'] - current_offset,\n",
    "        'end': annotation['end'] - current_offset,\n",
    "        'label': annotation['label'],\n",
    "        'text': annotation['text']\n",
    "    }\n",
    "    annotated_messages[message_number]['annotations'].append(offset_annotation)\n",
    "\n",
    "len(annotated_messages)\n"
   ]
  },
  {
   "source": [
    "This is necessary because the data was a bit mangled for some reason - some line breaks are lost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# s = re.sub(r'\\(\\.\\*\\?\\)([A-Z])', r'(.*?)\\n\\1', s)\n",
    "\n",
    "for message in annotated_messages:\n",
    "    message['text'] = re.sub(r'\\(\\.\\*\\?\\)([A-Z])', r'(.*?)\\n\\1', message['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_messages = []\n",
    "for message in annotated_messages:\n",
    "    pos = 0\n",
    "    num_splits = 0\n",
    "    while '\\n' in message['text'][pos:]:\n",
    "        new_pos = message['text'].find('\\n', pos)\n",
    "        new_s = message['text'][pos:new_pos]\n",
    "        new_a = [\n",
    "            ann.copy() for ann in message['annotations']\n",
    "                if ann['start'] >= pos - num_splits and ann['end'] <= new_pos - num_splits\n",
    "        ]\n",
    "        for annotation in new_a:\n",
    "            annotation['start'] -= pos - num_splits\n",
    "            annotation['end'] -= pos - num_splits\n",
    "        pos = new_pos + 1\n",
    "        new_messages.append({\n",
    "            'text': new_s,\n",
    "            'annotations': new_a\n",
    "        })\n",
    "        num_splits += 1\n",
    "\n",
    "    new_a = [ann.copy() for ann in message['annotations'] if ann['start'] >= pos - num_splits]\n",
    "    for annotation in new_a:\n",
    "        annotation['start'] -= pos - num_splits\n",
    "        annotation['end'] -= pos - num_splits\n",
    "    new_messages.append({\n",
    "        'text': message['text'][pos:],\n",
    "        'annotations': new_a\n",
    "    })\n",
    "\n",
    "annotated_messages = new_messages\n"
   ]
  },
  {
   "source": [
    "Now we want annotations to correspond to tokens instead of string positions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BatchEncoding\n",
    "from tokenizers import Encoding\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "tokenizer.add_tokens(\"(.*?)\")\n",
    "text = [message['text'] for message in annotated_messages]\n",
    "tokenized_batch: BatchEncoding = tokenizer(text, padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[CLS] — O\nNon — B-Filler\n- — I-Filler\nzero — I-Filler\nreturn — I-Filler\ncode — I-Filler\nfrom — I-Filler\ngenerate — I-Filler\n( — I-Filler\n97 — I-Filler\n) — I-Filler\n; — L-Filler\nLo — B-Filler\n##g — I-Filler\n##fi — I-Filler\n##le — I-Filler\nerror — I-Filler\nin — I-Filler\nlog — I-Filler\n. — I-Filler\ngenerate — I-Filler\n: — L-Filler\n\" — O\nTest — B-Error place\n##H — I-Error place\n##ep — I-Error place\n##MC — L-Error place\nFA — B-Error severity\n##TA — I-Error severity\n##L — L-Error severity\nThe — B-Error description\nefficiency — I-Error description\nafter — I-Error description\n101 — I-Error description\nevents — I-Error description\nis — I-Error description\n(.*?) — I-Error description\n! — I-Error description\n! — I-Error description\n! — L-Error description\n\" — O\n[SEP] — O\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n[PAD] — [PAD]\n"
     ]
    }
   ],
   "source": [
    "def align_annotations(tokenized_text: Encoding, annotations):\n",
    "    tokens = tokenized_text.tokens\n",
    "    labels = ['O'] * len([token for token in tokens if token != '[PAD]'])\n",
    "    labels.extend(['[PAD]'] * len([token for token in tokens if token == '[PAD]']))\n",
    "    for anno in annotations:\n",
    "        annotation_token_set = set()\n",
    "        for char_ix in range(anno['start'], anno['end']):\n",
    "            token_ix = tokenized_text.char_to_token(char_ix)\n",
    "            if token_ix is not None:\n",
    "                annotation_token_set.add(token_ix)\n",
    "        if len(annotation_token_set) == 1:\n",
    "            token_ix = annotation_token_set.pop()\n",
    "            labels[token_ix] = f\"U-{anno['label']}\"\n",
    "        else:\n",
    "            prefixes = ['B'] + ['I'] * (len(annotation_token_set) - 2) + ['L']\n",
    "            for prefix, token_ix in zip(prefixes, sorted(annotation_token_set)):\n",
    "                labels[token_ix] = f\"{prefix}-{anno['label']}\"\n",
    "    return labels\n",
    "\n",
    "labels = [\n",
    "    align_annotations(\n",
    "        tokenized_batch[i],\n",
    "        annotated_messages[i]['annotations']\n",
    "    ) for i in range(len(annotated_messages))\n",
    "]\n",
    "\n",
    "for token, label in zip(tokenized_batch[0].tokens, labels[0]):\n",
    "    print(f\"{token} — {label}\")\n"
   ]
  },
  {
   "source": [
    "This is a way to convert labels to numbers and back"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "all_labels = set(anno['label'] for message in annotated_messages for anno in message['annotations'])\n",
    "all_labels = ['[PAD]', 'O'] + [f\"{prefix}-{label}\" for prefix, label in product('BILU', all_labels)]\n",
    "label_to_id = {label: i for i, label in enumerate(all_labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}"
   ]
  },
  {
   "source": [
    "At this point we have:\n",
    " - `labels`\n",
    " - `tokenized_batch`\n",
    " - `text`\n",
    "\n",
    "and we want all those in the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69125e7c18b042c284e36aee74610801"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def encode_dataset(dataset):\n",
    "    return tokenizer(dataset['text'], padding=True)\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    'text': text,\n",
    "    'tokens': [tokenized_batch[i].tokens for i in range(len(text))],\n",
    "    'text_labels': labels,\n",
    "    'labels': [[label_to_id.get(l) for l in lab] for lab in labels]\n",
    "})\n",
    "\n",
    "dataset = dataset.map(encode_dataset, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'labels', 'text', 'text_labels', 'token_type_ids', 'tokens'],\n",
       "    num_rows: 809\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-875ff1bc0ee3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m trainer = Trainer(\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;31m# Model parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_parallel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    669\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    670\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, BertConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "model_config = BertConfig(vocab_size=30523)\n",
    "model = BertForTokenClassification(model_config)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./BERT_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}