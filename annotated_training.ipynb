{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotating the messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.cuda import is_available\n",
    "\n",
    "is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're writing the patterns into a file for further annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns = []\n",
    "# for pattern_list in result['patterns']:\n",
    "#     for pattern in pattern_list:\n",
    "#         patterns.append(pattern)\n",
    "\n",
    "# with open(\"../datasets/patterns.txt\", 'w') as pattern_file:\n",
    "#     for pattern in patterns[:20]:\n",
    "#         pattern_file.write(pattern.strip() + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we need to open the file with annotations and transform it into an adequate form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "708"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from json import loads\n",
    "with open('../datasets/admin1.jsonl', 'r') as annotated_file:\n",
    "    data = loads(annotated_file.read())\n",
    "\n",
    "raw_annotations = data['label']\n",
    "annotations = []\n",
    "for annotation in raw_annotations:\n",
    "    start, end, label = annotation\n",
    "    annotations.append({'start': start, 'end': end, 'label': label, 'text': data['data'][start:end]})\n",
    "\n",
    "annotations = sorted(annotations, key=lambda x: x['start'])\n",
    "split_content = data['data'].split('\\n')\n",
    "\n",
    "current_offset = 0\n",
    "next_offset = len(split_content[0]) + 1\n",
    "message_number = 0\n",
    "annotated_messages = [{'text': text, 'annotations': []} for text in split_content]\n",
    "for annotation in annotations:\n",
    "    if annotation['start'] >= next_offset and message_number < len(split_content) - 1:\n",
    "        message_number += 1\n",
    "        current_offset = next_offset\n",
    "        next_offset += len(split_content[message_number]) + 1\n",
    "    offset_annotation = {\n",
    "        'start': annotation['start'] - current_offset,\n",
    "        'end': annotation['end'] - current_offset,\n",
    "        'label': annotation['label'],\n",
    "        'text': annotation['text']\n",
    "    }\n",
    "    annotated_messages[message_number]['annotations'].append(offset_annotation)\n",
    "\n",
    "len(annotated_messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is necessary because the data was a bit mangled for some reason - some line breaks are lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# s = re.sub(r'\\(\\.\\*\\?\\)([A-Z])', r'(.*?)\\n\\1', s)\n",
    "\n",
    "for message in annotated_messages:\n",
    "    message['text'] = re.sub(r'\\(\\.\\*\\?\\)([A-Z])', r'(.*?)\\n\\1', message['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_messages = []\n",
    "for message in annotated_messages:\n",
    "    pos = 0\n",
    "    num_splits = 0\n",
    "    while '\\n' in message['text'][pos:]:\n",
    "        new_pos = message['text'].find('\\n', pos)\n",
    "        new_s = message['text'][pos:new_pos]\n",
    "        new_a = [\n",
    "            ann.copy() for ann in message['annotations']\n",
    "                if ann['start'] >= pos - num_splits and ann['end'] <= new_pos - num_splits\n",
    "        ]\n",
    "        for annotation in new_a:\n",
    "            annotation['start'] -= pos - num_splits\n",
    "            annotation['end'] -= pos - num_splits\n",
    "        pos = new_pos + 1\n",
    "        new_messages.append({\n",
    "            'text': new_s,\n",
    "            'annotations': new_a\n",
    "        })\n",
    "        num_splits += 1\n",
    "\n",
    "    new_a = [ann.copy() for ann in message['annotations'] if ann['start'] >= pos - num_splits]\n",
    "    for annotation in new_a:\n",
    "        annotation['start'] -= pos - num_splits\n",
    "        annotation['end'] -= pos - num_splits\n",
    "    new_messages.append({\n",
    "        'text': message['text'][pos:],\n",
    "        'annotations': new_a\n",
    "    })\n",
    "\n",
    "annotated_messages = new_messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want annotations to correspond to tokens instead of string positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BatchEncoding\n",
    "from tokenizers import Encoding\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "tokenizer.add_tokens(\"(.*?)\")\n",
    "text = [message['text'] for message in annotated_messages]\n",
    "tokenized_batch: BatchEncoding = tokenizer(text, padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] — O\n",
      "Non — B-Meaningless\n",
      "- — I-Meaningless\n",
      "zero — I-Meaningless\n",
      "return — I-Meaningless\n",
      "code — I-Meaningless\n",
      "from — I-Meaningless\n",
      "generate — I-Meaningless\n",
      "( — I-Meaningless\n",
      "97 — I-Meaningless\n",
      ") — L-Meaningless\n",
      "; — O\n",
      "Lo — B-Meaningless\n",
      "##g — I-Meaningless\n",
      "##fi — I-Meaningless\n",
      "##le — I-Meaningless\n",
      "error — I-Meaningless\n",
      "in — I-Meaningless\n",
      "log — I-Meaningless\n",
      ". — I-Meaningless\n",
      "generate — L-Meaningless\n",
      ": — O\n",
      "\" — O\n",
      "Test — B-Meaningful\n",
      "##H — I-Meaningful\n",
      "##ep — I-Meaningful\n",
      "##MC — I-Meaningful\n",
      "FA — I-Meaningful\n",
      "##TA — I-Meaningful\n",
      "##L — I-Meaningful\n",
      "The — I-Meaningful\n",
      "efficiency — I-Meaningful\n",
      "after — I-Meaningful\n",
      "101 — I-Meaningful\n",
      "events — I-Meaningful\n",
      "is — I-Meaningful\n",
      "(.*?) — I-Meaningful\n",
      "! — I-Meaningful\n",
      "! — I-Meaningful\n",
      "! — L-Meaningful\n",
      "\" — O\n",
      "[SEP] — O\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n",
      "[PAD] — [PAD]\n"
     ]
    }
   ],
   "source": [
    "def align_annotations(tokenized_text: Encoding, annotations):\n",
    "    tokens = tokenized_text.tokens\n",
    "    labels = ['O'] * len([token for token in tokens if token != '[PAD]'])\n",
    "    labels.extend(['[PAD]'] * len([token for token in tokens if token == '[PAD]']))\n",
    "    for anno in annotations:\n",
    "        annotation_token_set = set()\n",
    "        for char_ix in range(anno['start'], anno['end']):\n",
    "            token_ix = tokenized_text.char_to_token(char_ix)\n",
    "            if token_ix is not None:\n",
    "                annotation_token_set.add(token_ix)\n",
    "        if len(annotation_token_set) == 1:\n",
    "            token_ix = annotation_token_set.pop()\n",
    "            labels[token_ix] = f\"U-{anno['label']}\"\n",
    "        else:\n",
    "            prefixes = ['B'] + ['I'] * (len(annotation_token_set) - 2) + ['L']\n",
    "            for prefix, token_ix in zip(prefixes, sorted(annotation_token_set)):\n",
    "                labels[token_ix] = f\"{prefix}-{anno['label']}\"\n",
    "    return labels\n",
    "\n",
    "labels = [\n",
    "    align_annotations(\n",
    "        tokenized_batch[i],\n",
    "        annotated_messages[i]['annotations']\n",
    "    ) for i in range(len(annotated_messages))\n",
    "]\n",
    "\n",
    "for token, label in zip(tokenized_batch[0].tokens, labels[0]):\n",
    "    print(f\"{token} — {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a way to convert labels to numbers and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "all_labels = set(anno['label'] for message in annotated_messages for anno in message['annotations'])\n",
    "all_labels = ['[PAD]', 'O'] + [f\"{prefix}-{label}\" for prefix, label in product('BILU', all_labels)]\n",
    "label_to_id = {label: i for i, label in enumerate(all_labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have:\n",
    " - `labels`\n",
    " - `tokenized_batch`\n",
    " - `text`\n",
    "\n",
    "and we want all those in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee9765aad4b463aac48689f9162700e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def encode_dataset(dataset):\n",
    "    return tokenizer(dataset['text'], padding=True)\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    'text': text,\n",
    "    'tokens': [tokenized_batch[i].tokens for i in range(len(text))],\n",
    "    'text_labels': labels,\n",
    "    'labels': [[label_to_id.get(l) for l in lab] for lab in labels]\n",
    "})\n",
    "\n",
    "dataset = dataset.map(encode_dataset, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'labels', 'text', 'text_labels', 'token_type_ids', 'tokens'],\n",
       "     num_rows: 709\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'labels', 'text', 'text_labels', 'token_type_ids', 'tokens'],\n",
       "     num_rows: 100\n",
       " }))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.shuffle()\n",
    "train_dataset, eval_dataset = Dataset.from_dict(dataset[100:]), Dataset.from_dict(dataset[:100])\n",
    "train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2' max='2127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/2127 : < :, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2127, training_loss=0.22381831582970163)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, BertConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "model_config = BertConfig(vocab_size=30523, num_labels=len(all_labels))\n",
    "model = BertForTokenClassification(model_config)\n",
    "model.max_seq_length = 150\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./BERT_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    no_cuda=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}